{"cells":[{"metadata":{"_uuid":"2c207e76bece57f6a99439c61229a1a68d4db600"},"cell_type":"markdown","source":"## 1. **Importing the libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #for plotting\nimport seaborn as sea #for visualization\n\n# Set a few plotting defaults\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.size'] = 15\nplt.rcParams['patch.edgecolor'] = 'k'\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n# Suppress warnings from pandas\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n# Any results you write to the current directory are saved as output.p","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2b0b82dcbe0197903a9d4f60668c64118822f22"},"cell_type":"markdown","source":"## **2. Importing/exploring the train/test datasets and converting to numeric form**"},{"metadata":{"trusted":true,"_uuid":"8de9709857437e6d20fc97d9ee8ea8e298dbcede"},"cell_type":"code","source":"#let's look at all available files:\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\nprint (\"Train Dataset: Rows, Columns: \", train_df.shape)\nprint (\"Test Dataset: Rows, Columns: \", test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00d13717877e6756b66160b0a2616c27eabfc28c"},"cell_type":"code","source":"# a glimpse at train_df\ntrain_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e69f8013e8f3cc0944731569a705689ab74aeacc"},"cell_type":"code","source":"#First, let's deal with non-numeric columns\ntrain_df.select_dtypes(['object']).head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0bbfbe85e22de74f84839b33c85eb72a24c907be"},"cell_type":"code","source":"#Id and idhogar won't be used for training so we'll take care of them later\n#1. 'dependency'\ntrain_df['dependency'].value_counts(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Notice there is a column containing squared values for dependency, 'SQBdependency'. \n#see what are its analogs to 'yes' and 'no' of 'dependency':\nprint (train_df.loc[train_df['dependency']=='no',['SQBdependency']]['SQBdependency'].value_counts())\nprint (train_df.loc[train_df['dependency']=='yes',['SQBdependency']]['SQBdependency'].value_counts())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8968b2773fc2593c33c5623ebfe6959a98499f8"},"cell_type":"code","source":"#Convert 'yes' to 1 and 'no' to 0\ntrain_df['dependency'] = train_df['dependency'].replace(('yes', 'no'), (1, 0))\ntest_df['dependency'] = test_df['dependency'].replace(('yes', 'no'), (1, 0))\ntrain_df['dependency']=train_df['dependency'].astype(float)\ntest_df['dependency']=test_df['dependency'].astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0d6240817d1799b3f7161f30fb1b5407521fe50"},"cell_type":"code","source":"#2 and #3 'edjefe'/'edjefa'\ntrain_df['edjefe'].value_counts()\n#edjefe, years of education of male head of household, \n#based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92e60fbf4c863d92c90802988ad54430dfd86386"},"cell_type":"code","source":"train_df['edjefa'].value_counts()\n#edjefa, years of education of female head of household, \n#based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Again, correlate 'edjefe' with 'SQBedjefe'(squared value)\nprint (train_df.loc[train_df['edjefe']=='no',['SQBedjefe']]['SQBedjefe'].value_counts())\nprint (train_df.loc[train_df['edjefe']=='yes',['SQBedjefe']]['SQBedjefe'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a5e392e1cf9168bd00b47b8bb8e94a42aef1bee"},"cell_type":"code","source":"#Based on 'SQBedjefe' column, convert 'no' to 0 and 'yes' to 1 to make the rows of 'edjefa'/'edjefe' numeric\ntrain_df['edjefa'] = train_df['edjefa'].replace(('yes', 'no'), (1, 0))\ntrain_df['edjefe'] = train_df['edjefe'].replace(('yes', 'no'), (1, 0))\ntest_df['edjefa'] = test_df['edjefa'].replace(('yes', 'no'), (1, 0))\ntest_df['edjefe'] = test_df['edjefe'].replace(('yes', 'no'), (1, 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f20ac4e9a3dce9191a6f0a678d07e624b1f366f"},"cell_type":"code","source":"#converting these object type columns to floats\ntrain_df['edjefa']=train_df['edjefa'].astype(float)\ntrain_df['edjefe']=train_df['edjefe'].astype(float)\ntest_df['edjefa']=test_df['edjefa'].astype(float)\ntest_df['edjefe']=test_df['edjefe'].astype(float)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04d7e0afd3e377616674d5ac30a3e92c9c932ed7"},"cell_type":"code","source":"#double checking that all columns are now numeric - except for Id and idhogar\nprint (train_df.select_dtypes(['object']).describe(), '\\n')\nprint (test_df.select_dtypes(['object']).describe())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"105225123b5b2f727d1362e518bc9e6e54e5b0ae"},"cell_type":"markdown","source":"\n## 3. Taking care of the missing values"},{"metadata":{"trusted":true,"_uuid":"8530be791a45ac9bddc39261c33bfcd511f99f20"},"cell_type":"code","source":"#Now let's take care of the missing columns\nprint (\"Top Training Columns having missing values:\")\nmissing_df = train_df.isnull().sum().to_frame()\nmissing_df = missing_df.sort_values(0, ascending = False)\nmissing_df.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Top Testing Columns having missing values:\")\nmissing_df = test_df.isnull().sum().to_frame()\nmissing_df = missing_df.sort_values(0, ascending = False)\nmissing_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad53a035a42b7bf86141673f9a9026519915f163"},"cell_type":"code","source":"#1 'v18q1' - number of tablets household owns\ntrain_df.groupby('v18q')['v18q1'].apply(lambda x: x.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f0dd17a64f1b036e67a7256df79828c42467a31"},"cell_type":"code","source":"#Every family that has nan for v18q1 does not own a tablet. \n#Therefore, we can fill in this missing value with zero.\ntrain_df['v18q1'] = train_df['v18q1'].fillna(0)\ntest_df['v18q1'] = test_df['v18q1'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2 'rez_esc' - Years behind in school \n#let's see if high percentage of missing values in 'rez_esc' accounts for minors and people without education\nprint (train_df.loc[train_df['rez_esc'].isnull()]['age'].value_counts().head(6))\nprint (train_df.loc[train_df['rez_esc'].isnull()]['instlevel1'].value_counts())\nprint (train_df.loc[train_df['rez_esc'].isnull()]['instlevel2'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#another theory is that those 'na' are for individuals outside of school age\nprint (train_df.loc[train_df['rez_esc'].notnull()]['age'].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e9a4b5b903c4266510e14f20788bb0ae6c35b08"},"cell_type":"code","source":"#which is actually true: min age - 7, max age - 17. Assigning '0' to those people\ntrain_df['rez_esc'] = train_df['rez_esc'].fillna(0)\ntest_df['rez_esc'] = test_df['rez_esc'].fillna(0)\ntrain_df.loc[train_df['rez_esc'] > 5, 'rez_esc'] = 5\ntest_df.loc[test_df['rez_esc'] > 5, 'rez_esc'] = 5 #5 is a maximum value per competition's discussion, so here we're accounting for the outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcb4cb406b71872a922ba4bb8cb299ff76d992bf"},"cell_type":"code","source":"#3 v2a1, Monthly rent payment\nprint(train_df['v2a1'].isnull().sum())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7649ea2fccf0530cb9a3c5ee81f3dd36dffd5a98"},"cell_type":"code","source":"#Let's correlate it with tipovivi1, =1 own and fully paid house\nprint (train_df.loc[train_df['v2a1'].isnull()]['tipovivi1'].value_counts())\nprint(train_df['tipovivi1'].value_counts())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Replacing with '0' na for fully paid house \ntrain_df.loc[(train_df['v2a1'].isnull() & train_df['tipovivi1'] == 1), 'v2a1'] = 0\ntest_df.loc[(test_df['v2a1'].isnull() & test_df['tipovivi1'] == 1), 'v2a1'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (train_df.loc[train_df['v2a1'].isnull()]['tipovivi1'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tipovivi2, \"=1 own,  paying in installments\"\n#tipovivi3, =1 rented\n#tipovivi4, =1 precarious\n#tipovivi5, \"=1 other(assigned,  borrowed)\nprint (train_df.loc[train_df['v2a1'].isnull()]['tipovivi2'].value_counts())\nprint (train_df.loc[train_df['v2a1'].isnull()]['tipovivi3'].value_counts())\nprint (train_df.loc[train_df['v2a1'].isnull()]['tipovivi4'].value_counts())\nprint (train_df.loc[train_df['v2a1'].isnull()]['tipovivi5'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's replace na for precarious with '0' as well\ntrain_df.loc[(train_df['v2a1'].isnull() & train_df['tipovivi4'] == 1), 'v2a1'] = 0\ntest_df.loc[(test_df['v2a1'].isnull() & test_df['tipovivi4'] == 1), 'v2a1'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (train_df.loc[train_df['v2a1'].isnull()]['Target'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#see if we can find a feature to correlate with those remaining missing values\nv2a1_na_corr = train_df\nv2a1_na_corr.v2a1.where(v2a1_na_corr.v2a1.isnull(), 1, inplace=True)\nv2a1_na_corr['v2a1'].fillna(0, inplace = True)\nprint (v2a1_na_corr.corr()['v2a1'].sort_values())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1539b3f2afaadfdb8d48baef748db94af21c5d9b"},"cell_type":"code","source":"#No luck. But since the property is 'assigned, borrowed', let's assume there's no monthly rent associated with it\ntrain_df['v2a1'].fillna(train_df['v2a1'].mean(), inplace = True)\ntest_df['v2a1'].fillna(test_df['v2a1'].mean(), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Top Training Columns having missing values:\")\nmissing_df = train_df.isnull().sum().to_frame()\nmissing_df = missing_df.sort_values(0, ascending = False)\nprint (missing_df.head())\nprint (\"Top Testing Columns having missing values:\")\nmissing_df = test_df.isnull().sum().to_frame()\nmissing_df = missing_df.sort_values(0, ascending = False)\nprint (missing_df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9457a4ef21a6cebf1711e97d880d393f019e61e0"},"cell_type":"code","source":"#the rest of the missing values can be replaced with mean as their percentage towards total number of entries is insignificant\ntrain_df.fillna (train_df.mean(), inplace = True)\ntest_df.fillna(test_df.mean(), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"861ada1e42b53afa995ada3af1d2d431a5ab207c"},"cell_type":"code","source":"print ('Columns having missing values:')\nprint (train_df.columns[train_df.isnull().any()])\nprint (test_df.columns[test_df.isnull().any()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making sure the targets are calculated based on head of the household values\ntrain_df = train_df.loc[train_df['parentesco1'] == 1]\ntrain_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ed8a34d958fe0c74d9b813cf0ab0e7b73d6c241"},"cell_type":"markdown","source":"## **4. Data visuzalisation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Target'].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#top 30 features with best correlation to 'Target'\nbest_correlations = train_df.corr()['Target'].abs().sort_values().tail(30)\ntype(best_correlations)\nbest_correlations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_correlation = best_correlations.index\nbest_correlation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = {'dependency':'dependency, Dependency rate', 'v18q1':'v18q1, number of tablets household owns', 'epared1':'epared1, if walls are bad', 'qmobilephone':'qmobilephone, # of mobile phones', \n     'pisocemento':'pisocemento, =1 if predominant material on the floor is cement',\n       'eviv1':'eviv1, =1 if floor are bad', 'instlevel8':'instlevel8, =1 undergraduate and higher education', 'rooms':'rooms,  number of all rooms in the house', 'r4h1':'r4h1, Males younger than 12 years of age', \n        'v18q': 'v18q, owns a tablet:', 'edjefe':'edjefe, years of education of male head of household', 'SQBedjefe':'SQBedjefe, years of education of male head of household squared',\n       'etecho3':'etecho3, =1 if roof are good', 'r4m1':'r4m1, Females younger than 12 years of age', 'SQBovercrowding':'SQBovercrowding, overcrowding squared', \n       'paredblolad':'paredblolad, =1 if predominant material on the outside wall is block or brick', 'SQBmeaned':'SQBmeaned, square of the mean years of education of adults (>=18) in the household',\n       'pisomoscer':'pisomoscer, \"=1 if predominant material on the floor is mosaic,  ceramic,  terrazo\"', 'overcrowding':'overcrowding, # persons per room', 'epared3':'epared3, =1 if walls are good',\n        'eviv3':'eviv3, =1 if floor are good', 'SQBescolari' :'SQBescolari, years of schooling squared',\n       'escolari':'escolari, years of schooling', 'cielorazo':'cielorazo, =1 if the house has ceiling', 'SQBhogar_nin':'SQBhogar_nin, Number of children 0 to 19 in household, squared',\n        'r4t1':'r4t1, persons younger than 12 years of age', 'hogar_nin':'Number of children 0 to 19 in household',\n       'meaneduc':'meaneduc,average years of education for adults (18+)', 'Target':'Target', 'elimbasu5':'elimbasu5, \"=1 if rubbish disposal mainly by throwing in river,  creek or sea\"'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#features with <5 possible values\nfor j in best_correlation:\n    if len(train_df[j].unique())<5:\n        sea.countplot(x=j, hue='Target', data=train_df)\n        plt.xlabel(d.get(j))\n        plt.ylabel(\"Count\")\n        #plt.title(str(j),' vs Target') \n        plt.figure()\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#if target distribution in each feature cathegory is similar to overall target distribution, \n#then the chances are that the feature will have a better correlation to Target\n#I tried to combine a couple of features to produce better distributions/correlations\ntrain_df['v18q+etecho3'] = train_df['v18q']+train_df['etecho3']\nprint (train_df.corr()['Target']['v18q'])\nprint (train_df.corr()['Target']['etecho3'])\nprint (train_df.corr()['Target']['v18q+etecho3'])\ntest_df['v18q+etecho3'] = test_df['v18q']+test_df['etecho3']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['v18q+paredblolad'] = train_df['v18q']+train_df['paredblolad']\nprint (train_df.corr()['Target']['v18q'])\nprint (train_df.corr()['Target']['paredblolad'])\nprint (train_df.corr()['Target']['v18q+paredblolad'])\ntest_df['v18q+paredblolad'] = test_df['v18q']+test_df['paredblolad']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['v18q+pisomoscer'] = train_df['v18q']+train_df['pisomoscer']\nprint (train_df.corr()['Target']['v18q'])\nprint (train_df.corr()['Target']['pisomoscer'])\nprint (train_df.corr()['Target']['v18q+pisomoscer'])\ntest_df['v18q+pisomoscer'] = test_df['v18q']+test_df['pisomoscer']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['pisomoscer+instlevel8'] = train_df['pisomoscer']+train_df['instlevel8']\nprint (train_df.corr()['Target']['pisomoscer'])\nprint (train_df.corr()['Target']['instlevel8'])\nprint (train_df.corr()['Target']['pisomoscer+instlevel8'])\ntest_df['pisomoscer+instlevel8'] = test_df['pisomoscer']+test_df['instlevel8']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_distribution(df, var, target, **kwargs):\n    row = kwargs.get('row', None)\n    col = kwargs.get('col', None)\n    facet = sea.FacetGrid(df, hue = target, size=4.0, aspect=1.3, sharex=False, sharey=False)\n    facet.map(sea.kdeplot, var)\n    facet.set(xlim = (0, df[var].max()))\n    facet.add_legend()\n    plt.xlabel(d.get(j))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#features with >=5 possible values\nfor j in best_correlation:\n    if len(train_df[j].unique())>5:\n        plot_distribution(train_df, j, 'Target')\n\n#In the first graph instead of 0's should be nulls(we changed these before). So there is no info about monthly rate payment for non vulnerable households ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#following the same logic, let's try to combine features to get a better distribution\ntrain_df['edjefe+escolari'] = train_df['edjefe']+train_df['escolari']\nprint (train_df.corr()['Target']['edjefe'])\nprint (train_df.corr()['Target']['escolari'])\nprint (train_df.corr()['Target']['edjefe+escolari'])\ntest_df['edjefe+escolari'] = test_df['edjefe']+test_df['escolari']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18ca1f43e2f5c4d180c92c5b12c2c48eaa523fe5"},"cell_type":"markdown","source":"## **5. Feature Engineering**"},{"metadata":{},"cell_type":"markdown","source":"### Manual feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"#poor materials used\ntrain_df[\"Poor_materials\"]=train_df['pareddes']+train_df['paredfibras']+train_df['pisonatur']+train_df['pisonotiene']+train_df['techocane']+train_df['epared1']+train_df['etecho1']+train_df['eviv1']\ntest_df[\"Poor_materials\"]=test_df['pareddes']+test_df['paredfibras']+test_df['pisonatur']+test_df['pisonotiene']+test_df['techocane']+test_df['epared1']+test_df['etecho1']+test_df['eviv1']\nprint ('Pearson correlation coefficients:')\nprint ('Poor Materials (training set): ',train_df['Poor_materials'].corr( train_df['Target']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#rich materials used\ntrain_df[\"Rich_Materials\"]=train_df['paredblolad']+train_df['pisomoscer']+train_df['techoentrepiso']+train_df['techootro']+train_df['cielorazo']+train_df['epared3']+train_df['etecho3']+train_df['eviv3']\ntest_df[\"Rich_Materials\"]=test_df['paredblolad']+test_df['pisomoscer']+test_df['techoentrepiso']+test_df['techootro']+test_df['cielorazo']+test_df['epared3']+test_df['etecho3']+test_df['eviv3']\nprint ('Pearson correlation coefficients:')\nprint ('Materials (training set): ',train_df['Rich_Materials'].corr( train_df['Target']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"Poor_Infrastructure\"]=train_df['abastaguano']+train_df['noelec']+train_df['epared1']+train_df['etecho1']+train_df['eviv1']+train_df['lugar3']+train_df['sanitario1']+train_df['energcocinar1']+train_df['elimbasu3']\ntest_df[\"Poor_Infrastructure\"]=test_df['abastaguano']+test_df['noelec']+test_df['epared1']+test_df['etecho1']+test_df['eviv1']+test_df['lugar3']+test_df['sanitario1']+test_df['energcocinar1']+test_df['elimbasu3']\nprint ('Pearson correlation coefficients:')\nprint ('Materials (training set): ',train_df['Poor_Infrastructure'].corr( train_df['Target']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"Good_Infrastructure\"]=train_df['sanitario2']+train_df['energcocinar2']+train_df['elimbasu1']+train_df['abastaguadentro']+train_df['planpri']+train_df['epared3']+train_df['etecho3']*(3)+train_df['eviv3']+train_df['lugar1']+train_df['lugar2']+train_df['lugar6']\ntest_df[\"Good_Infrastructure\"]=test_df['sanitario2']+test_df['energcocinar2']+test_df['elimbasu1']+test_df['abastaguadentro']+test_df['planpri']+test_df['epared3']+test_df['etecho3']*(3)+test_df['eviv3']+test_df['lugar1']+test_df['lugar2']+test_df['lugar6']\nprint ('Pearson correlation coefficients:')\nprint ('Infrastructure (training set): ',train_df['Good_Infrastructure'].corr( train_df['Target']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#overcrowding + total of persons younger than 12 years of age + no level of education + zona rural\ntrain_df[\"overcrowding_total\"] = train_df[\"hacdor\"]+train_df[\"r4t1\"] +train_df[\"instlevel1\"] + train_df[\"area2\"]\ntest_df[\"overcrowding_total\"] = test_df[\"hacdor\"]+ test_df[\"r4t1\"] + test_df[\"instlevel1\"] + test_df[\"area2\"]\nprint ('overcrowding_total: ',train_df['overcrowding_total'].corr( train_df['Target']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#years of schooling + overcdrowding\ntrain_df[\"escolari+hacapo\"] = train_df[\"escolari\"]+train_df[\"hacapo\"]\ntest_df[\"escolari+hacapo\"] = test_df[\"escolari\"]+test_df[\"hacapo\"]\nprint (train_df['escolari+hacapo'].corr( train_df['Target']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = test_df[['Id']]\ntrain_df = train_df.select_dtypes(exclude=['object'])\ntest_df = test_df.select_dtypes(exclude=['object'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## #6. Dimension reduction."},{"metadata":{"trusted":true,"_uuid":"bc32aebe3a9ad6b011b5a06bcd87dd6fb3f055a1"},"cell_type":"code","source":"#Removing columns with greater than 99% correlation as redundant\n# Create correlation matrix\ncorr_matrix = train_df.corr()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.99\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.99)]\n\nprint(f'There are {len(to_drop)} correlated columns to remove.')\nprint(to_drop)\n\ntrain_df = train_df.drop(columns = to_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be873f80a42f842fa0a5bf0918bb616c235b1cf3"},"cell_type":"code","source":"#let's compare all the correlation coefficients to see if some new variable can be created\nprint (train_df.corr()['Target'].sort_values().head(-30))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5e48fa636e55260a20ccdae0ad90bd24b144dcb"},"cell_type":"code","source":"#realligning two datasets based on the features selected in training\ntrain_df_H20 = train_df # for use with autoML\ny_df = train_df['Target']\ntrain_df, test_df = train_df.align(test_df, join = 'inner', axis = 1)\nprint(f\"Training set shape:{train_df.shape}, testing set shape:{test_df.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9de734fdf88dfb328d6cb6b9f28894b9c8b57bd"},"cell_type":"markdown","source":"## **7. Converting to numpy arrays and scaling**"},{"metadata":{"trusted":true,"_uuid":"38d13dcc45b892f0b612c44a9381f8994585c028"},"cell_type":"code","source":"#converting to numpy array\nX = train_df.values\ny = y_df.values\ny = y.reshape(-1, 1)\ntest_np = test_df.values\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"140ceb74f21ac4a1fc76a517eef046c513c68359"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split (X, y,test_size = 0.05, random_state = 123)\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8acd29fb35fe3d3b3db0000d9c141668f70b1016"},"cell_type":"code","source":"#Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\nX = sc.transform(X)\nprint (X)\ntest_np = sc.transform (test_np)\nprint (test_np)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c568ea4fe4b96d46ddd2f2f681124524c844221"},"cell_type":"markdown","source":"## **8. Building the model**"},{"metadata":{"trusted":true,"_uuid":"050c6f76339e7d34cd3f895ca76f069223a2ed6b"},"cell_type":"code","source":"#credits to https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro for the parameters values\nimport lightgbm as lgb\nclassifier = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n                             random_state=None, silent=True, metric='None', \n                             n_jobs=4, n_estimators=5000, class_weight='balanced',\n                             colsample_bytree =  0.93, min_child_samples = 95, num_leaves = 14, subsample = 0.96)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdb80d84da7432163a14e9c4373fe4c001e45cb3"},"cell_type":"code","source":"eval_set = [(X_train, y_train), (X_test, y_test)]\nclassifier.fit(X_train, y_train, eval_metric=\"multiclass\", eval_set=eval_set, verbose=True, early_stopping_rounds=500) #LGBoost model model\ny_pred = classifier.predict(X_test) \ny_pred = y_pred.reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3367900d7a6b568b7882f1f9b232fbc884ec539"},"cell_type":"code","source":"# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm1 = confusion_matrix(y_test, y_pred)\nprint (cm1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04b7f08a1eb8009a127025b7652b4ab9e6a32b94"},"cell_type":"code","source":"from sklearn.metrics import f1_score\nf1_1 = f1_score(y_test, y_pred, average ='macro')\nprint ('f1 score for LGBoost model:',f1_1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9. Making prediction "},{"metadata":{"trusted":true,"_uuid":"261298d8d91eceaf6eb5464934f70fefc796d48e"},"cell_type":"code","source":"y_pred = classifier.predict(test_np)\ny_pred = y_pred.reshape(-1, 1)\ny_pred = y_pred.astype(int)\nprint(plt.hist(y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 10. Visualizing/explaining the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualise with a barplot\nimport seaborn as sns\nindices = np.argsort(classifier.feature_importances_)[::-1]\nindices = indices[:20]\n\n\nplt.subplots(figsize=(40, 40))\ng = sea.barplot(y=train_df.columns[indices], x = classifier.feature_importances_[indices], orient='h')\ng.set_xlabel(\"Relative importance\",fontsize=40)\ng.set_ylabel(\"Features\",fontsize=40)\ng.tick_params(labelsize=40)\ng.set_title(\"Feature importance\", fontsize=40)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 11.Submitting to the competition"},{"metadata":{"trusted":true,"_uuid":"7606fa8969e3bfab5facd20d2cb9a3ce77635d0d"},"cell_type":"code","source":"#Submitting the prediction\nsubmit['TARGET'] = y_pred\nsubmit.head()   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5cd0ae1ce08e3c02dc3d71c8deb8ea608ef92a66"},"cell_type":"code","source":"submit.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the submission to a csv file\nsubmit.to_csv('LGBClassification.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}